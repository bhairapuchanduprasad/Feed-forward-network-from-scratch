{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1867159",
   "metadata": {},
   "source": [
    "# Linear regression\n",
    "\n",
    "In the first part of the assignment we will implement the foward pass and the backward local and global gradient computation for a simple linear regression problem with mean squared error loss.\n",
    "\n",
    "Our decomposition of the linear regression corresponds to the middle computation graph in slide 11 of Lecture 04 with 2 compute nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d6f0ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary initialization\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "979a3fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances: 90, input dimensions: 3.\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "from ann_code.helpers import load_data\n",
    "in_data, labels = load_data(filename='./ann_data/toy_data.csv') # correct filename if necessary\n",
    "# get data dimensions\n",
    "num_inst, num_dim = in_data.shape\n",
    "print(f\"Number of instances: {num_inst}, input dimensions: {num_dim}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798266a9",
   "metadata": {},
   "source": [
    "## 1) Linear regression - single data point\n",
    "\n",
    "To make things easy, we first work over a single data example.\n",
    "\n",
    "The prediction function is an affine function (linear with bias) with parameters $\\theta = {\\mathbf{w}, b}$.\n",
    "We write it here in full detail to see the individual scalar parameters (elements of vector $\\mathbf{w}$)\n",
    "\n",
    "$$\\hat{y} = f_\\theta(\\mathbf{x}) = \\sum_{j=1}^d x_j \\, w_j + b \\enspace .$$\n",
    "\n",
    "The loss is the squared error (SE)\n",
    "\n",
    "$$\\mathcal{L}_{SE}(\\hat{y}, y) = (\\hat{y} - y)^2 \\enspace .$$\n",
    "\n",
    "Work with the code in `code/linear_regression.py` and complete it as instructed here below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b5c4016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([ 0.8872, -1.2852,  0.3707]), \n",
      "y: tensor([0.2390])\n"
     ]
    }
   ],
   "source": [
    "# get single data example\n",
    "x = in_data[0, :]\n",
    "y = labels[0]\n",
    "print(f\"x: {x}, \\ny: {y}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fe1270",
   "metadata": {},
   "source": [
    "### Forward propagation\n",
    "\n",
    "I have implemented for you the forward pass using `for` loops to calculate the inner product in the `linear_single_forward` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97b4ced4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([ 0.8872, -1.2852,  0.3707]) \n",
      "w: tensor([ 1.5410, -0.2934, -2.1788]), \n",
      "b: tensor([0.8380])\n",
      "Prediction tensor([1.7745])\n"
     ]
    }
   ],
   "source": [
    "# get prediciton using the provided function linear_single_forward\n",
    "from ann_code.linear_regression import linear_single_forward\n",
    "\n",
    "# initialize parameters w, b\n",
    "w = torch.tensor([ 1.5410, -0.2934, -2.1788]) \n",
    "b = torch.tensor([0.8380])\n",
    "\n",
    "# print data and parameters for info\n",
    "print(f\"x: {x} \\nw: {w}, \\nb: {b}\")\n",
    "\n",
    "# get predictions\n",
    "yhat, lin_cache = linear_single_forward(x, w, b)\n",
    "print(f\"Prediction {yhat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd2e1f1",
   "metadata": {},
   "source": [
    "I have also implemented the `squared_error_forward` function for you to check the accuracy of the predcition against the true label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91044e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Squarred error: tensor([2.3577])\n",
      "(tensor([1.7745]), tensor([0.2390]))\n"
     ]
    }
   ],
   "source": [
    "# get squred error using the provided function squared_error_forward\n",
    "from ann_code.linear_regression import squared_error_forward\n",
    "\n",
    "# calcualte squred error\n",
    "loss, loss_cache = squared_error_forward(yhat, y)\n",
    "print(f\"Squarred error: {loss}\")\n",
    "print(loss_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb6b6ee",
   "metadata": {},
   "source": [
    "### Backward propagation\n",
    "\n",
    "You will now implement the backward propagation functions. \n",
    "\n",
    "#### Local gradients\n",
    "\n",
    "Remember that each computation node needs to be able to compute its local gradient before it can combine it with the upstream gradient through the chain rule.\n",
    "Local gradients are simply gradients of the function outputs with respect to its inputs, so far ignoring or possible upstream or downstream operations.\n",
    "\n",
    "Derive the local gradients and implement them in `linear_single_lgrad` and `squared_error_lgrad`.\n",
    "Then use the cell bellow to check your implementation. \n",
    "The relative errors should all be rather small (e.g. 1e-4).\n",
    "\n",
    "Note 1: I use the term *gradient* for simplicity even though for scalar objects *derivative* would be more appropriate.\n",
    "\n",
    "Note 2: We will get the gradients with respect to all inputs of the functions including the data $\\mathbf{x}$ and $y$. It will become clear why these may be useful in later parts of the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16ffefc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.0710])\n",
      "Checking xg\n",
      "analytic grad: tensor([ 1.5410, -0.2934, -2.1788])\n",
      "numerical grad: tensor([ 1.5408, -0.2933, -2.1791], grad_fn=<ViewBackward0>)\n",
      "relative error: tensor([9.6748e-08, 4.2130e-08, 2.3903e-07], grad_fn=<MulBackward0>)\n",
      "Checking wg\n",
      "analytic grad: tensor([ 0.8872, -1.2852,  0.3707])\n",
      "numerical grad: tensor([ 0.8875, -1.2851,  0.3695], grad_fn=<ViewBackward0>)\n",
      "relative error: tensor([1.8547e-07, 1.3927e-08, 2.8144e-06], grad_fn=<MulBackward0>)\n",
      "Checking bg\n",
      "analytic grad: tensor([1.])\n",
      "numerical grad: tensor([1.0002], grad_fn=<ViewBackward0>)\n",
      "relative error: tensor([5.5072e-08], grad_fn=<MulBackward0>)\n",
      "Checking yhatg\n",
      "analytic grad: tensor([3.0710])\n",
      "numerical grad: tensor([3.0708], grad_fn=<ViewBackward0>)\n",
      "relative error: tensor([4.2442e-08], grad_fn=<MulBackward0>)\n",
      "Checking yg\n",
      "analytic grad: tensor([-3.0710])\n",
      "numerical grad: tensor([-3.0708], grad_fn=<ViewBackward0>)\n",
      "relative error: tensor([4.2442e-08], grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:173: UserWarning: CUDA initialization: CUDA driver initialization failed, you might not have a CUDA gpu. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\c10\\cuda\\CUDAFunctions.cpp:112.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    }
   ],
   "source": [
    "# After implementing the local gradient functions, you can check them here\n",
    "from ann_code.helpers import numerical_gradient, grad_checker\n",
    "from ann_code.linear_regression import linear_single_lgrad, squared_error_lgrad\n",
    "yhatg=torch.zeros(1)\n",
    "# get local gradients of the linear function\n",
    "xg, wg, bg = linear_single_lgrad(lin_cache)\n",
    "\n",
    "# get local gradients of the squared error\n",
    "yhatg, yg = squared_error_lgrad(loss_cache)\n",
    "print(yhatg)\n",
    "# check local gradients\n",
    "# check xg\n",
    "print(f\"Checking xg\")\n",
    "f = lambda theta: linear_single_forward(theta, w, b)[0]\n",
    "xng = numerical_gradient(f, x)\n",
    "grad_checker(xg, xng)\n",
    "\n",
    "# check wg\n",
    "print(f\"Checking wg\")\n",
    "f = lambda theta: linear_single_forward(x, theta, b)[0]\n",
    "wng = numerical_gradient(f, w)\n",
    "grad_checker(wg, wng)\n",
    "\n",
    "# check bg\n",
    "print(f\"Checking bg\")\n",
    "f = lambda theta: linear_single_forward(x, w, theta)[0]\n",
    "bng = numerical_gradient(f, b)\n",
    "grad_checker(bg, bng)\n",
    "\n",
    "# check yhatg\n",
    "print(f\"Checking yhatg\")\n",
    "f = lambda theta: squared_error_forward(theta, y)[0]\n",
    "yhatng = numerical_gradient(f, yhat)\n",
    "grad_checker(yhatg, yhatng)\n",
    "\n",
    "# check bg\n",
    "print(f\"Checking yg\")\n",
    "f = lambda theta: squared_error_forward(yhat, theta)[0]\n",
    "yng = numerical_gradient(f, y)\n",
    "grad_checker(yg, yng)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f9e7f4",
   "metadata": {},
   "source": [
    "#### Global gradients\n",
    "\n",
    "To get the global gradients you need to use the chain rule - each local gradient needs to be mulitplied with the upstream global gradient.\n",
    "Implement the global gradient calcualtion in `linear_single_ggrad`. Remember that the local and global gradient of the loss function are equal so there is no need to write a specific function for these.\n",
    "The relative errors should all be rather small (e.g. 1e-4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed77ca3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking xgrad\n",
      "analytic grad: tensor([ 4.7324, -0.9010, -6.6910])\n",
      "numerical grad: tensor([ 4.7314, -0.9012, -6.6924], grad_fn=<ViewBackward0>)\n",
      "relative error: tensor([1.8391e-06, 7.8083e-08, 3.7249e-06], grad_fn=<MulBackward0>)\n",
      "Checking wgrad\n",
      "analytic grad: tensor([ 2.7246, -3.9467,  1.1385])\n",
      "numerical grad: tensor([ 2.7251, -3.9458,  1.1349], grad_fn=<ViewBackward0>)\n",
      "relative error: tensor([5.5576e-07, 1.5071e-06, 2.6592e-05], grad_fn=<MulBackward0>)\n",
      "Checking bgrad\n",
      "analytic grad: tensor([3.0710])\n",
      "numerical grad: tensor([3.0708], grad_fn=<ViewBackward0>)\n",
      "relative error: tensor([4.2442e-08], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# After implementing the global gradient functions, you can check them here\n",
    "from ann_code.linear_regression import linear_single_ggrad\n",
    "\n",
    "# get global gradients of the linear function parameters w, b and data x\n",
    "xgrad, wgrad, bgrad = linear_single_ggrad((xg, wg, bg), yhatg)\n",
    "\n",
    "# check global gradients\n",
    "# check xgrad\n",
    "print(f\"Checking xgrad\")\n",
    "f = lambda theta: squared_error_forward(linear_single_forward(theta, w, b)[0], y)[0]\n",
    "xng = numerical_gradient(f, x)\n",
    "grad_checker(xgrad, xng)\n",
    "\n",
    "# check wgrad\n",
    "print(f\"Checking wgrad\")\n",
    "f = lambda theta: squared_error_forward(linear_single_forward(x, theta, b)[0], y)[0]\n",
    "wng = numerical_gradient(f, w)\n",
    "grad_checker(wgrad, wng)\n",
    "\n",
    "# check bgrad\n",
    "print(f\"Checking bgrad\")\n",
    "f = lambda theta: squared_error_forward(linear_single_forward(x, w, theta)[0], y)[0]\n",
    "bng = numerical_gradient(f, b)\n",
    "grad_checker(bgrad, bng)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2df0de",
   "metadata": {},
   "source": [
    "## 2) Linear regression - vectorize and refactor\n",
    "\n",
    "We will now vectorize our code (drop for loops), make it work for a whole batch of examples in one go, and merge the local and global gradient calculations into one step.\n",
    "\n",
    "For a set of $n$ examples with inputs $\\mathbf{x} \\in \\mathbb{R}^{d}$ and outpus $\\mathbf{y} \\in \\mathbb{R}^n$ the affine (linear with bias) prediciton function with parameters $\\theta = {\\mathbf{w}, b}$ is\n",
    "\n",
    "$$\\mathbf{\\hat{y}} = f_\\theta(\\mathbf{\\mathbf{X}}) = \\mathbf{Xw} + b \\enspace ,$$\n",
    "where $\\mathbf{X}$ is the $(n \\times d)$ data matrix and the scalar bias $b$ is broadcasted across the whole prediction vector.\n",
    "\n",
    "The loss is the mean squared error (MSE)\n",
    "\n",
    "$$\\mathcal{L}_{MSE}(\\mathbf{\\hat{y}, y}) = \\frac{1}{n} ||\\mathbf{\\hat{y}} - \\mathbf{y}||_2^2 \\enspace .$$\n",
    "\n",
    "Work with the code in `code/linear_regression.py` and complete it as instructed here below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3b8f53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: torch.Size([90, 3]), \n",
      "y: torch.Size([90, 1])\n"
     ]
    }
   ],
   "source": [
    "# get input and output data\n",
    "X = in_data\n",
    "y = labels\n",
    "w = torch.tensor([ 1.5410, -0.2934, -2.1788]) \n",
    "b = torch.tensor([0.8380])\n",
    "print(f\"X: {X.shape}, \\ny: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3f115b",
   "metadata": {},
   "source": [
    "### Forward propagation\n",
    "\n",
    "Implement the vectorized versions of the forward pass in `linear_forward` and `mse_forward`. \n",
    "Avoid using `for`, `while` or other lopps!\n",
    "\n",
    "Check your implementation by comparing your mse loss with the correct value (the differnce should be tiny)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3e8cef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your loss: 10.318559646606445, correct loss: 10.318559646606445\n"
     ]
    }
   ],
   "source": [
    "# get predicitons and mse loss using the implemented functions\n",
    "from ann_code.linear_regression import linear_forward, mse_forward\n",
    "\n",
    "# parameter values\n",
    "if w.dim() == 1:\n",
    "    w = w[:, None] # add dimensions to w to make it (d, 1) tensor\n",
    "b = b # same as above\n",
    "\n",
    "# get predictions (use the same parameter values as above)\n",
    "yhat, lin_cache = linear_forward(X, w, b)\n",
    "\n",
    "# get mse loss\n",
    "loss, loss_cache = mse_forward(yhat, y)\n",
    "\n",
    "print(f\"Your loss: {loss}, correct loss: 10.318559646606445\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d179b42",
   "metadata": {},
   "source": [
    "### Backward propagation\n",
    "\n",
    "Next you shall implement the backward pass. Instead of creating two separate functions for local and global gradient calculation, we will merge these into a single function for each compute node.\n",
    "\n",
    "Derive the local gradients and combine them appropriately with the upstream gradient to obtain the global gradient necessary for the backward propagation in `linear_backward` and `mse_backward`.\n",
    "\n",
    "Then use the cell bellow to check your implementation. \n",
    "The relative errors should all be rather small (e.g. 1e-4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cb1ac8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Xgrad\n",
      "To save space, printing only randomly selected elements:\n",
      "analytic grad: tensor([[-0.0846, -0.0019, -0.0705, -0.1331, -0.0728]])\n",
      "numerical grad: tensor([[-0.0858, -0.0048, -0.0715, -0.1335, -0.0763]])\n",
      "relative error: tensor([[3.1964e-06, 1.5998e-05, 2.2464e-06, 4.2331e-07, 2.4990e-05]])\n",
      "Checking wgrad\n",
      "To save space, printing only randomly selected elements:\n",
      "analytic grad: tensor([[ 4.8180, -4.8611,  4.8180,  4.8180, -4.8611]])\n",
      "numerical grad: tensor([[ 4.8256, -4.8542,  4.8256,  4.8256, -4.8542]])\n",
      "relative error: tensor([[1.1640e-04, 9.5097e-05, 1.1640e-04, 1.1640e-04, 9.5097e-05]])\n",
      "Checking bg\n",
      "analytic grad: 1.6759978532791138\n",
      "numerical grad: tensor([1.6785])\n",
      "relative error: tensor([1.2191e-05])\n"
     ]
    }
   ],
   "source": [
    "# After implementing the backward pass functions, you can check them here\n",
    "from ann_code.linear_regression import linear_backward, mse_backward\n",
    "\n",
    "# get mse gradients\n",
    "yhatgrad, ygrad = mse_backward(loss_cache)\n",
    "#print(\"yhatgrad====\",yhatgrad)\n",
    "# get linear func gradients\n",
    "Xgrad, wgrad, bgrad = linear_backward(lin_cache, yhatgrad)\n",
    "\n",
    "# check global gradients\n",
    "# check xgrad\n",
    "print(f\"Checking Xgrad\")\n",
    "f = lambda theta: mse_forward(linear_forward(theta, w, b)[0], y)[0]\n",
    "xng = numerical_gradient(f, X)\n",
    "grad_checker(Xgrad, xng, rnd=True)\n",
    "\n",
    "# check wgrad\n",
    "print(f\"Checking wgrad\")\n",
    "f = lambda theta: mse_forward(linear_forward(X, theta, b)[0], y)[0]\n",
    "wng = numerical_gradient(f, w)\n",
    "grad_checker(wgrad, wng, rnd=True)\n",
    "\n",
    "# check bgrad\n",
    "print(f\"Checking bg\")\n",
    "f = lambda theta: mse_forward(linear_forward(X, w, theta)[0], y)[0]\n",
    "bng = numerical_gradient(f, b)\n",
    "grad_checker(bgrad, bng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaede4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
